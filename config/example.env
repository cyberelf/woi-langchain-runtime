# LangChain Agent Runtime Configuration Example
# Copy this file to .env and customize with your actual values

# =============================================================================
# Server Configuration
# =============================================================================
HOST=0.0.0.0
PORT=8000
DEBUG=false
RELOAD=false

# =============================================================================
# Authentication
# =============================================================================
RUNTIME_TOKEN=rt-1234567890abcdef  # Change this to a secure token

# =============================================================================
# LLM Provider API Keys
# =============================================================================
# OpenAI
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic
ANTHROPIC_API_KEY=sk-ant-api-your-key-here

# DeepSeek
DEEPSEEK_API_KEY=sk-your-deepseek-key-here

# Google AI
GOOGLE_API_KEY=your-google-ai-key-here

# Azure OpenAI (if using)
AZURE_OPENAI_API_KEY=your-azure-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Local LLM (e.g., Ollama, vLLM)
LOCAL_LLM_BASE_URL=http://localhost:11434/v1

# =============================================================================
# LLM Proxy Configuration (if using a proxy service)
# =============================================================================
LLM_PROXY_URL=http://localhost:8001
LLM_PROXY_TOKEN=proxy-token-here

# =============================================================================
# Runtime Limits & Performance
# =============================================================================
MAX_CONCURRENT_AGENTS=100
MAX_MESSAGE_LENGTH=32000
MAX_CONVERSATION_HISTORY=100
TASK_MANAGER_WORKERS=10
TASK_CLEANUP_INTERVAL=3600
INSTANCE_TIMEOUT=7200

# =============================================================================
# Execution Timeouts (seconds)
# =============================================================================
AGENT_EXECUTION_TIMEOUT=300
LLM_REQUEST_TIMEOUT=60

# =============================================================================
# Message Queue Configuration
# =============================================================================
MESSAGE_QUEUE_TYPE=memory  # Options: memory, redis, rabbitmq
REDIS_URL=redis://localhost:6379/0
RABBITMQ_URL=amqp://guest:guest@localhost:5672/

# =============================================================================
# Logging & Monitoring
# =============================================================================
LOG_LEVEL=INFO

# =============================================================================
# Services Configuration
# =============================================================================
# Option 1: Load from file (recommended)
SERVICES_CONFIG_FILE=config/services-config.json

# Option 2: Inline JSON string (fallback - not recommended for complex configs)
# SERVICES_CONFIG='{"llm":{"providers":{"openai":{"type":"openai","model":"gpt-4o-mini"}},"default_provider":"openai"}}'

# If both are set, SERVICES_CONFIG_FILE takes priority
