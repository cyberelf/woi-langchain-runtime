# LangChain Agent Runtime Configuration

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=false
RELOAD=false

# Authentication
RUNTIME_TOKEN=your-runtime-token-here

# LLM Proxy Configuration
LLM_PROXY_URL=http://localhost:8001
LLM_PROXY_TOKEN=your-llm-proxy-token

# OpenAI Configuration (Fallback)
OPENAI_API_KEY=your-openai-api-key
OPENAI_BASE_URL=https://api.openai.com/v1

# Runtime Limits
MAX_CONCURRENT_AGENTS=100
MAX_MESSAGE_LENGTH=32000
MAX_CONVERSATION_HISTORY=100

# Execution Timeouts (seconds)
AGENT_EXECUTION_TIMEOUT=300
LLM_REQUEST_TIMEOUT=60

# Logging
LOG_LEVEL=INFO 